# -*- coding: utf-8 -*-
"""Albin_Final_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16kdV4Mw1rPb9FlBldRHUmi4V93as6Rhc
"""

import pandas as pd
from google.colab import drive
# Loading an Excel file from Google Drive
drive.mount('/content/drive')
# Load the datasets
defensive_action = pd.read_excel('/content/drive/My Drive/defensive action stats.xlsx')
goal_creation = pd.read_excel('/content/drive/My Drive/goal and shot creation stat.xlsx')
passing_stats = pd.read_excel('/content/drive/My Drive/passing stats.xlsx')
player_wages = pd.read_excel('/content/drive/My Drive/Player wages.xlsx')
shooting_stats = pd.read_excel('/content/drive/My Drive/Shooting.xlsx')

selected_col_GCA= ['Player', 'SCA', 'SCA90', 'GCA90']
goal_creation_features = goal_creation[selected_col_GCA]
selected_col_def= ['Player', 'Pos', 'Squad', 'Age', 'Blocks', 'Sh', 'Pass', 'Int', 'Clr', 'Err']
defensive_action_features= defensive_action[selected_col_def]
selected_col_pass= ['Player', 'Cmp%']
passing_stats_features = passing_stats[selected_col_pass]
selected_col_shoot= ['Player', '90s', 'Gls', 'Sh', 'SoT', 'SoT%', 'Sh/90', 'SoT/90', 'G/Sh', 'G/SoT']
shooting_stats_features = shooting_stats[selected_col_shoot]
selected_col_wages= ['Player', 'Weekly Wages', 'Annual Wages']
player_wages_features = player_wages[selected_col_wages]
# Merge datasets on common columns (e.g., 'Player', 'Rk')
# Adjust the merge keys and method depending on the dataset's structure
player_dataset = defensive_action_features.merge(goal_creation_features, on=['Player'], how='outer') \
                            .merge(passing_stats_features, on=['Player'], how='outer') \
                            .merge(player_wages_features, on=['Player'], how='outer') \
                            .merge(shooting_stats_features, on=['Player'], how='outer')

player_dataset = player_dataset.drop_duplicates(subset=['Player'])

player_dataset = player_dataset.rename(columns={
    'Sh_x': 'Shots_blocked',
    'Pass': 'Pass_blocked',
    'Int': 'Interceptions',
    'Clr': 'clearance',
    'Err': 'Mistakes',
    'Cmp%': 'pass_completion_%',
    'Sh_y': 'Sh'
})

# Save the final table to a new Excel file
player_dataset.to_excel('/content/drive/My Drive/final_table.xlsx', index=False)
# Display the final dataframe
print(player_dataset.head())

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import ace_tools as tools


# Load the dataset
file_path = "/mnt/data/Final football player dataset.xlsx"
df = pd.read_excel(file_path)

# Exploratory Data Analysis (EDA)
# Checking basic statistics
eda_summary = df.describe()

# Visualize correlations between performance metrics and wages
plt.figure(figsize=(10, 6))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

# Histograms of key features
df.hist(bins=30, figsize=(15, 10))
plt.suptitle('Histograms of Key Features')
plt.show()

# Data Preparation for the Model
# Assuming 'Stay/Look for Option' as the target variable (Needs to be defined in the dataset)
df['Target'] = df.apply(lambda row: 1 if row['Performance Metric'] > threshold else 0, axis=1) # Define threshold logic

# Selecting features for the model
features = ['Performance Metric', 'Wages', 'Age', 'Position Code'] # Adjust features accordingly
X = df[features]
y = df['Target']

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Building the model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Predicting and evaluating the model
y_pred = model.predict(X_test)

# Output evaluation metrics
print("Classification Report:\n", classification_report(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

# Making predictions on the full dataset
df['Prediction'] = model.predict(X)

# Mapping predictions to human-readable form
df['Future'] = df['Prediction'].apply(lambda x: 'Stay' if x == 1 else 'Look for Better Option')

# Output to Excel
output_df = df[['Player Name', 'Position', 'Team', 'Future']]
output_file = "/mnt/data/Player_Future_Predictions.xlsx"
output_df.to_excel(output_file, index=False)

# Display the EDA summary to the user
tools.display_dataframe_to_user(name="EDA Summary", dataframe=eda_summary)

# Display basic info about the dataset
print("Basic Information:")
print(df.info())

print("\nDescriptive Statistics:")
print(df.describe())

# Automated EDA report
profile = ProfileReport(df, title="Professional Players EDA Report", explorative=True)
profile.to_file("Professional_Players_EDA_Report.html")
print("EDA Report saved as 'Professional_Players_EDA_Report.html'.")

# Cleaning: Handle missing values
df.fillna('N/A', inplace=True)

# Feature Engineering: Convert columns to numeric where appropriate
def convert_wage(value):
    try:
        if isinstance(value, str):
            return float(value.replace('â‚¬', '').replace(',', '').strip())
    except:
        return np.nan
    return value

if 'Weekly Wage' in df.columns and 'Market Value' in df.columns:
    df['Weekly Wage'] = df['Weekly Wage'].apply(convert_wage)
    df['Market Value'] = df['Market Value'].apply(convert_wage)
else:
    print("Error: Required columns 'Weekly Wage' or 'Market Value' are missing from the dataset.")

# Select relevant features for training
required_columns = ['Age', 'Appearances', 'Goals', 'Assists', 'Pass Accuracy', 'Tackles',
                    'Interceptions', 'Shots on Target', 'Minutes Played', 'Weekly Wage', 'Market Value']

missing_columns = [col for col in required_columns if col not in df.columns]
if missing_columns:
    print(f"Error: The following required columns are missing from the dataset: {missing_columns}")
    # Handle or exit, as necessary
else:
    X = df[required_columns]

# Assuming there is a 'Future' column indicating the prediction target
if 'Future' in df.columns:
    y = df['Future']
else:
    raise ValueError("The target column 'Future' is missing from the dataset.")

# Convert categorical data if necessary
X = pd.get_dummies(X, drop_first=True)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model Training: Random Forest Classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Predictions
y_pred = rf_model.predict(X_test)

# Evaluation
print("Classification Report for Random Forest Classifier:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
plt.figure(figsize=(10, 6))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix for Player Future Prediction')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Save predictions to Excel
if 'Player Name' in df.columns and 'Team' in df.columns:
    output = pd.DataFrame({'Player Name': df.loc[X_test.index, 'Player Name'],
                           'Team': df.loc[X_test.index, 'Team'],
                           'Future Prediction': y_pred})
    output.to_excel('Player_Future_Prediction.xlsx', index=False)
    print("Output saved to 'Player_Future_Prediction.xlsx'.")
else:
    print("Error: 'Player Name' or 'Team' columns are missing from the dataset.")

!pip install shap
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.feature_selection import SelectKBest, mutual_info_classif
import xgboost as xgb
import shap
import re

# Step 1: Data Preparation
# ----------------------------------------------
def convert_currency(value):
    if isinstance(value, str):
        value = re.sub(r'[^\d.]', '', value.split(' ')[1])  # Extract the number part and remove non-numeric characters
        return float(value) if value else np.nan
    return value

# Apply currency conversion to relevant columns
currency_columns = ['Weekly Wages', 'Annual Wages']  # Replace with your actual column names
for col in currency_columns:
    player_dataset[col] = player_dataset[col].apply(convert_currency)

# Check for non-numeric values and handle them
def convert_to_numeric(value):
    """
    Converts values to numeric if possible, or returns NaN for non-numeric values.
    """
    try:
        return pd.to_numeric(value)
    except ValueError:
        # Return NaN for any non-numeric value
        return np.nan

# Apply the function to all columns that should be numeric
for col in player_dataset.columns:
    if player_dataset[col].dtype == 'object':  # Check only object type columns
        # Check if '90s' or similar non-numeric values are present
        player_dataset[col] = player_dataset[col].replace('90s', np.nan)  # Replace '90s' with NaN
        # Convert column to numeric, coercing errors to NaN
        player_dataset[col] = player_dataset[col].apply(convert_to_numeric)

# Separate numeric and non-numeric columns
numeric_cols = player_dataset.select_dtypes(include=[np.number]).columns
non_numeric_cols = player_dataset.select_dtypes(exclude=[np.number]).columns

# Handle missing values for numeric columns by filling with mean
player_dataset[numeric_cols] = player_dataset[numeric_cols].fillna(player_dataset[numeric_cols].mean())

# Handle missing values for non-numeric columns by filling with mode
for col in non_numeric_cols:
    player_dataset[col] = player_dataset[col].fillna(player_dataset[col].mode()[0])

# Encode categorical features
label_encoder = LabelEncoder()
player_dataset['Pos'] = label_encoder.fit_transform(player_dataset['Pos'])
player_dataset['Squad'] = label_encoder.fit_transform(player_dataset['Squad'])

# Update numerical features list to only include columns that exist in the data
numerical_features = ['Age', 'SCA', 'SCA90', 'GCA90', 'Blocks', 'Interceptions', 'clearance', 'pass_completion_%', 'Weekly Wages', 'Annual Wages']
numerical_features = [feature for feature in numerical_features if feature in player_dataset.columns]  # Ensure columns exist

# Feature Scaling
scaler = StandardScaler()
player_dataset[numerical_features] = scaler.fit_transform(player_dataset[numerical_features])

# Feature Engineering
if 'Blocks' in player_dataset.columns and 'Interceptions' in player_dataset.columns and 'clearance' in player_dataset.columns:
    player_dataset['defensive_score'] = player_dataset['Blocks'] + player_dataset['Interceptions'] + player_dataset['clearance']
if 'SCA90' in player_dataset.columns and 'GCA90' in player_dataset.columns:
    player_dataset['offensive_score'] = player_dataset['SCA90'] + player_dataset['GCA90']

# Step 2: Assign New 'Future' Based on More Complex Criteria
# ----------------------------------------------
if 'offensive_score' in player_dataset.columns and 'defensive_score' in player_dataset.columns and 'Weekly Wages' in player_dataset.columns and 'Age' in player_dataset.columns:
    # Use percentiles to set thresholds
    offensive_threshold = np.percentile(player_dataset['offensive_score'], 75)  # Top 25% players
    defensive_threshold = np.percentile(player_dataset['defensive_score'], 75)  # Top 25% players
    wage_threshold = np.percentile(player_dataset['Weekly Wages'], 25)  # Bottom 25% wages
    age_threshold = np.percentile(player_dataset['Age'], 50)  # Median age

    # Assign 'look for better option' if a player is above thresholds for performance and below for wages
    player_dataset['future'] = np.where(
        ((player_dataset['offensive_score'] > offensive_threshold) |
         (player_dataset['defensive_score'] > defensive_threshold)) &
        (player_dataset['Weekly Wages'] < wage_threshold) &
        (player_dataset['Age'] < age_threshold),
        'look for better option',
        'stay in the club'
    )
else:
    print("Relevant columns not found, using alternative criteria.")
    player_dataset['future'] = np.where(player_dataset['Age'] < player_dataset['Age'].median(), 'look for better option', 'stay in the club')

# Check new target distribution
print("New target variable distribution after revised criteria:")
print(player_dataset['future'].value_counts())

# Encode target variable after reassignment
y = player_dataset['future'].apply(lambda x: 1 if x == 'look for better option' else 0)  # Encode target variable

# Continue with the rest of the steps (Feature Selection, Model Training, Evaluation, etc.) as before

# Step 5: Feature Selection using Mutual Information
# ----------------------------------------------
# Check if 'Player' and 'future' columns exist before dropping them
columns_to_drop = [col for col in ['Player', 'future'] if col in player_dataset.columns]

# Selecting top features using mutual information
X = player_dataset.drop(columns=columns_to_drop)  # Drop columns only if they exist

print("\nTarget variable distribution (after encoding):")
print(pd.Series(y).value_counts())  # Check distribution again

if y.nunique() <= 1:
    raise ValueError("Target variable has only one class after encoding. Please revise the 'future' assignment criteria.")

best_features = SelectKBest(score_func=mutual_info_classif, k='all')  # Use mutual information for feature selection
fit = best_features.fit(X, y)
dfscores = pd.DataFrame(fit.scores_)
dfcolumns = pd.DataFrame(X.columns)

# Concat two dataframes for better visualization
feature_scores = pd.concat([dfcolumns, dfscores], axis=1)
feature_scores.columns = ['Specs', 'Score']  # Naming the dataframe columns
print(feature_scores.nlargest(10, 'Score'))  # Print 10 best features

# Step 6: Model Building and Training
# ----------------------------------------------
# Split data into training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the model
model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)
model.fit(X_train, y_train)

# Step 7: Model Evaluation
# ----------------------------------------------
# Predict on test data
y_pred = model.predict(X_test)

# Model performance metrics
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))
print(f"Accuracy Score: {accuracy_score(y_test, y_pred):.4f}")

# Step 8: Model Interpretation using SHAP
# ----------------------------------------------
explainer = shap.Explainer(model)
shap_values = explainer(X_test)

# SHAP summary plot
shap.summary_plot(shap_values, X_test, plot_type="bar")

# Step 9: Adding Predictions to the Data
# ----------------------------------------------
# Predict future for the entire dataset
player_dataset['future'] = model.predict(X)

# Map the predictions back to "stay in the club" or "look for better options"
player_dataset['future'] = player_dataset['future'].apply(lambda x: 'look for better option' if x == 1 else 'stay in the club')

# Save the updated data to a new Excel file
output_path = "/content/drive/My Drive/player_future_predictions.xlsx"  # Changed path to ensure write access
player_dataset.to_excel(output_path, index=False)

print(f"Predictions have been saved to {output_path}")