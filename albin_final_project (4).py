# -*- coding: utf-8 -*-
"""Albin_Final_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16kdV4Mw1rPb9FlBldRHUmi4V93as6Rhc
"""

!pip install shap
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import xgboost as xgb
import shap
import re
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.feature_selection import SelectKBest, mutual_info_classif
from google.colab import drive

# Loading an Excel file from Google Drive
drive.mount('/content/drive')
# Load the datasets
defensive_action = pd.read_excel('/content/drive/My Drive/defensive action stats.xlsx')
goal_creation = pd.read_excel('/content/drive/My Drive/goal and shot creation stat.xlsx')
passing_stats = pd.read_excel('/content/drive/My Drive/passing stats.xlsx')
player_wages = pd.read_excel('/content/drive/My Drive/Player wages.xlsx')
shooting_stats = pd.read_excel('/content/drive/My Drive/Shooting.xlsx')

selected_col_GCA= ['Player', 'SCA', 'SCA90', 'GCA90']
goal_creation_features = goal_creation[selected_col_GCA]
selected_col_def= ['Player', 'Pos', 'Squad', 'Age', 'Blocks', 'Sh', 'Pass', 'Int', 'Clr', 'Err']
defensive_action_features= defensive_action[selected_col_def]
selected_col_pass= ['Player', 'Cmp%']
passing_stats_features = passing_stats[selected_col_pass]
selected_col_shoot= ['Player', '90s', 'Gls', 'Sh', 'SoT', 'SoT%', 'Sh/90', 'SoT/90', 'G/Sh', 'G/SoT']
shooting_stats_features = shooting_stats[selected_col_shoot]
selected_col_wages= ['Player', 'Weekly Wages', 'Annual Wages']
player_wages_features = player_wages[selected_col_wages]
# Merge datasets on common columns (e.g., 'Player', 'Rk')
# Adjust the merge keys and method depending on the dataset's structure
player_dataset = defensive_action_features.merge(goal_creation_features, on=['Player'], how='outer') \
                            .merge(passing_stats_features, on=['Player'], how='outer') \
                            .merge(player_wages_features, on=['Player'], how='outer') \
                            .merge(shooting_stats_features, on=['Player'], how='outer')

player_dataset = player_dataset.drop_duplicates(subset=['Player'])

player_dataset = player_dataset.rename(columns={
    'Sh_x': 'Shots_blocked',
    'Pass': 'Pass_blocked',
    'Int': 'Interceptions',
    'Clr': 'clearance',
    'Err': 'Mistakes',
    'Cmp%': 'pass_completion_%',
    'Sh_y': 'Sh'
})

player_dataset.replace([np.inf, -np.inf], np.nan, inplace=True)
player_dataset.fillna(0, inplace=True)

# Save the final table to a new Excel file
player_dataset.to_excel('/content/drive/My Drive/final_table.xlsx', index=False)

# Display the final dataframe
print(player_dataset.head())

# Exploratory Data Analysis (EDA)
# Checking basic statistics
eda_summary = player_dataset.describe()
numeric_cols = player_dataset.select_dtypes(include=[np.number])
# Visualize correlations between performance metrics and wages
plt.figure(figsize=(10, 6))
sns.heatmap(numeric_cols.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()


# Histograms of key features
player_dataset.hist(bins=30, figsize=(15, 10))
plt.suptitle('Histograms of Key Features')
plt.show()

# Display basic info about the dataset
print("Basic Information:")
print(player_dataset.info())

print("\nDescriptive Statistics:")
print(player_dataset.describe())

# Define the function to convert the 'Annual Wages' column
def convert_pound_wage(value):
    """
    Convert a wage value string with currency symbols and commas to a numeric float.
    Extracts the British Pound (£) value and converts it to a float. If conversion fails, return NaN.
    """
    try:
        if isinstance(value, str) and '£' in value:
            # Extract the value after the British Pound symbol (£)
            pound_value = value.split('£')[1].split('(')[0]
            return float(pound_value.replace(',', '').strip())
    except (ValueError, IndexError):  # Specific exceptions for conversion or extraction failure
        return np.nan
    return value

# Apply the conversion function to the 'Annual Wages' column
player_dataset['Annual Wages'] = player_dataset['Annual Wages'].apply(convert_pound_wage)
player_dataset['Weekly Wages'] = player_dataset['Weekly Wages'].apply(convert_pound_wage)

# **Identify the correct target variable column name. If it's not 'Future', replace it below.**
# Replace 'YOUR_TARGET_COLUMN' with the actual name of your target variable column
if 'Pos' in player_dataset.columns:  # Example: Assuming 'Pos' (player position) is the target
    y = player_dataset['Pos']
else:
    raise ValueError(f"The target column 'Pos' is missing from the dataset.")

# Convert categorical data if necessary
X = pd.get_dummies(X, drop_first=True)

#RANDOMFOREST
# Cleaning: Handle missing values
# Fill missing values with appropriate strategies
# For numeric columns, you can use mean, median, or 0
# For categorical columns, you can use the most frequent value or a placeholder like 'Unknown'
player_dataset['Weekly Wages'] = player_dataset['Weekly Wages'].fillna(player_dataset['Weekly Wages'].median())
player_dataset['Annual Wages'] = player_dataset['Annual Wages'].fillna(player_dataset['Annual Wages'].median())
# ... (fill other columns as needed)

# Feature Engineering: Convert columns to numeric where appropriate
def convert_wage(value):
    try:
        if isinstance(value, str):
            return float(value.replace('€', '').replace(',', '').strip())
        else:
            return value  # Handle cases where value is already numeric
    except:
        return np.nan

if 'Weekly Wages' in player_dataset.columns and 'Annual Wages' in player_dataset.columns:
    player_dataset['Weekly Wages'] = player_dataset['Weekly Wages'].apply(convert_wage)
    player_dataset['Annual Wages'] = player_dataset['Annual Wages'].apply(convert_wage)
else:
    print("Error: Required columns 'Weekly Wage' or 'Annual Wages' are missing from the dataset.")

# ... (rest of the code)

# Before training the model, check for remaining NaN values
print(player_dataset.isnull().sum())

# Convert target variable to numeric if it's categorical
if y.dtype == object:  # Check if 'y' is of object type (likely string)
    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    # Convert all values in 'y' to strings before applying LabelEncoder
    y = y.astype(str)
    y = le.fit_transform(y)

# Before training the model, check for remaining NaN values
print(player_dataset.isnull().sum())

# If there are still NaN values, consider imputation or dropping rows with missing values
# Imputation (replace NaN with estimated values)
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='median')  # Choose an appropriate strategy
X = imputer.fit_transform(X)

# Alternatively, drop rows with NaN values
# X = X.dropna()

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model Training: Random Forest Classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Predictions
y_pred = rf_model.predict(X_test)

# Evaluation
print("Classification Report for Random Forest Classifier:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
plt.figure(figsize=(10, 6))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix for Player Future Prediction')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Save predictions to Excel
if 'Player' in player_dataset.columns and 'Team' in player_dataset.columns:
    output = pd.DataFrame({'Player Name': player_dataset.loc[X_test.index, 'Player Name'],
                           'Team': player_dataset.loc[X_test.index, 'Team'],
                           'Future Prediction': y_pred})
    output.to_excel('Player_Future_Prediction.xlsx', index=False)
    print("Output saved to 'Player_Future_Prediction.xlsx'.")
else:
    print("Error: 'Player Name' or 'Team' columns are missing from the dataset.")

# Step 1: Data Preparation
# ----------------------------------------------
def convert_currency(value):
    if isinstance(value, str):
        value = re.sub(r'[^\d.]', '', value.split(' ')[1])  # Extract the number part and remove non-numeric characters
        return float(value) if value else np.nan
    return value

# Apply currency conversion to relevant columns
currency_columns = ['Weekly Wages', 'Annual Wages']  # Replace with your actual column names
for col in currency_columns:
    player_dataset[col] = player_dataset[col].apply(convert_currency)

# Check for non-numeric values and handle them
def convert_to_numeric(value):
    """
    Converts values to numeric if possible, or returns NaN for non-numeric values.
    """
    try:
        return pd.to_numeric(value)
    except ValueError:
        # Return NaN for any non-numeric value
        return np.nan

# Apply the function to all columns that should be numeric
for col in player_dataset.columns:
    if player_dataset[col].dtype == 'object':  # Check only object type columns
        # Check if '90s' or similar non-numeric values are present
        player_dataset[col] = player_dataset[col].replace('90s', np.nan)  # Replace '90s' with NaN
        # Convert column to numeric, coercing errors to NaN
        player_dataset[col] = player_dataset[col].apply(convert_to_numeric)

# Separate numeric and non-numeric columns
numeric_cols = player_dataset.select_dtypes(include=[np.number]).columns
non_numeric_cols = player_dataset.select_dtypes(exclude=[np.number]).columns

# Handle missing values for numeric columns by filling with mean
player_dataset[numeric_cols] = player_dataset[numeric_cols].fillna(player_dataset[numeric_cols].mean())

# Handle missing values for non-numeric columns by filling with mode
for col in non_numeric_cols:
    player_dataset[col] = player_dataset[col].fillna(player_dataset[col].mode()[0])

# Encode categorical features
label_encoder = LabelEncoder()
player_dataset['Pos'] = label_encoder.fit_transform(player_dataset['Pos'])
player_dataset['Squad'] = label_encoder.fit_transform(player_dataset['Squad'])

# Update numerical features list to only include columns that exist in the data
numerical_features = ['Age', 'SCA', 'SCA90', 'GCA90', 'Blocks', 'Interceptions', 'clearance', 'pass_completion_%', 'Weekly Wages', 'Annual Wages']
numerical_features = [feature for feature in numerical_features if feature in player_dataset.columns]  # Ensure columns exist

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.feature_selection import SelectKBest, mutual_info_classif
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
import xgboost as xgb
import shap

# Feature Scaling
scaler = StandardScaler()
player_dataset[numerical_features] = scaler.fit_transform(player_dataset[numerical_features])

# Feature Engineering: Creating additional scores
if 'Blocks' in player_dataset.columns and 'Interceptions' in player_dataset.columns and 'clearance' in player_dataset.columns:
    player_dataset['defensive_score'] = player_dataset['Blocks'] + player_dataset['Interceptions'] + player_dataset['clearance']
if 'SCA90' in player_dataset.columns and 'GCA90' in player_dataset.columns:
    player_dataset['offensive_score'] = player_dataset['SCA90'] + player_dataset['GCA90']

# Assign New 'Future' Based on More Complex Criteria
offensive_threshold = np.percentile(player_dataset['offensive_score'], 75)
defensive_threshold = np.percentile(player_dataset['defensive_score'], 75)
wage_threshold = np.percentile(player_dataset['Weekly Wages'], 25)
age_threshold = np.percentile(player_dataset['Age'], 50)

# Relax the criteria to see if it changes the target distribution
player_dataset['future'] = np.where(
    ((player_dataset['offensive_score'] > offensive_threshold) |
     (player_dataset['defensive_score'] > defensive_threshold)) &
    (player_dataset['Weekly Wages'] <= wage_threshold) &
    (player_dataset['Age'] <= age_threshold),
    'look for better option',
    'stay in the club'
)

# Check new target distribution
print("New target variable distribution after revised criteria:")
print(player_dataset['future'].value_counts())

# Encode target variable after reassignment
y = player_dataset['future'].apply(lambda x: 1 if x == 'look for better option' else 0)  # Encode target variable

# Feature Selection using Mutual Information
columns_to_drop = [col for col in ['Player', 'future'] if col in player_dataset.columns]
X = player_dataset.drop(columns=columns_to_drop)  # Drop columns only if they exist

print("\nTarget variable distribution (after encoding):")
print(pd.Series(y).value_counts())  # Check distribution again

if y.nunique() <= 1:
    raise ValueError("Target variable has only one class after encoding. Please revise the 'future' assignment criteria.")

# Selecting top features using mutual information
best_features = SelectKBest(score_func=mutual_info_classif, k='all')  # Use mutual information for feature selection
fit = best_features.fit(X, y)
dfscores = pd.DataFrame(fit.scores_)
dfcolumns = pd.DataFrame(X.columns)

# Concat two dataframes for better visualization
feature_scores = pd.concat([dfcolumns, dfscores], axis=1)
feature_scores.columns = ['Specs', 'Score']  # Naming the dataframe columns
print(feature_scores.nlargest(10, 'Score'))  # Print 10 best features

# Split data into training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the model
model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)

# Cross-validation setup
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')

print(f"Cross-Validation Accuracy Scores: {cv_scores}")
print(f"Mean Cross-Validation Accuracy: {cv_scores.mean():.4f}")

# Train the model on the full training data
model.fit(X_train, y_train)

# Predict on test data
y_pred = model.predict(X_test)

# Model performance metrics
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))
print(f"Accuracy Score: {accuracy_score(y_test, y_pred):.4f}")

# SHAP Analysis
explainer = shap.Explainer(model)
shap_values = explainer(X_test)

# SHAP summary plot
shap.summary_plot(shap_values, X_test, plot_type="bar")

# Predict future for the entire dataset
player_dataset['future'] = model.predict(X)

# Map the predictions back to "stay in the club" or "look for better options"
player_dataset['future'] = player_dataset['future'].apply(lambda x: 'look for better option' if x == 1 else 'stay in the club')

# Save the updated data to a new Excel file
output_path = '/content/drive/My Drive/player_future_predictions.xlsx'  # Updated path to suit the context
player_dataset.to_excel(output_path, index=False)

print(f"Predictions have been saved to {output_path}")

#XGradientBoosting
# Step 6: Model Building and Training
# ----------------------------------------------
# Split data into training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the model
model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)
model.fit(X_train, y_train)

# Step 7: Model Evaluation
# ----------------------------------------------
# Predict on test data
y_pred = model.predict(X_test)

# Model performance metrics
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))
print(f"Accuracy Score: {accuracy_score(y_test, y_pred):.4f}")

# Step 8: Model Interpretation using SHAP
# ----------------------------------------------
explainer = shap.Explainer(model)
shap_values = explainer(X_test)

# SHAP summary plot
shap.summary_plot(shap_values, X_test, plot_type="bar")

# Step 9: Adding Predictions to the Data
# ----------------------------------------------
# Predict future for the entire dataset
player_dataset['future'] = model.predict(X)

# Map the predictions back to "stay in the club" or "look for better options"
player_dataset['future'] = player_dataset['future'].apply(lambda x: 'look for better option' if x == 1 else 'stay in the club')

# Save the updated data to a new Excel file
output_path = "/content/drive/My Drive/player_future_predictions.xlsx"  # Changed path to ensure write access
player_dataset.to_excel(output_path, index=False)

print(f"Predictions have been saved to {output_path}")