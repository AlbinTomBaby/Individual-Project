# -*- coding: utf-8 -*-
"""Albin_Final_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16kdV4Mw1rPb9FlBldRHUmi4V93as6Rhc
"""

!pip install shap
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import xgboost as xgb
import shap
import re
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.feature_selection import SelectKBest, mutual_info_classif
from google.colab import drive
from sklearn.impute import SimpleImputer
from sklearn.base import BaseEstimator, ClassifierMixin

# Loading an Excel file from Google Drive
drive.mount('/content/drive')
# Loading the datasets
defensive_action = pd.read_excel('/content/drive/My Drive/defensive action stats.xlsx')
goal_creation = pd.read_excel('/content/drive/My Drive/goal and shot creation stat.xlsx')
passing_stats = pd.read_excel('/content/drive/My Drive/passing stats.xlsx')
player_wages = pd.read_excel('/content/drive/My Drive/Player wages.xlsx')
shooting_stats = pd.read_excel('/content/drive/My Drive/Shooting.xlsx')

#Selecting the required columns
selected_col_GCA= ['Player', 'SCA', 'SCA90', 'GCA90']
goal_creation_features = goal_creation[selected_col_GCA]
selected_col_def= ['Player', 'Pos', 'Squad', 'Age', 'Blocks', 'Sh', 'Pass', 'Int', 'Clr', 'Err']
defensive_action_features= defensive_action[selected_col_def]
selected_col_pass= ['Player', 'Cmp%']
passing_stats_features = passing_stats[selected_col_pass]
selected_col_shoot= ['Player', '90s', 'Gls', 'Sh', 'SoT', 'SoT%', 'Sh/90', 'SoT/90', 'G/Sh', 'G/SoT']
shooting_stats_features = shooting_stats[selected_col_shoot]
selected_col_wages= ['Player', 'Weekly Wages', 'Annual Wages']
player_wages_features = player_wages[selected_col_wages]
# Merging the datasets and removing duplicates
player_dataset = defensive_action_features.merge(goal_creation_features, on=['Player'], how='outer') \
                            .merge(passing_stats_features, on=['Player'], how='outer') \
                            .merge(player_wages_features, on=['Player'], how='outer') \
                            .merge(shooting_stats_features, on=['Player'], how='outer')

player_dataset = player_dataset.drop_duplicates(subset=['Player'])

#Renaming column name
player_dataset = player_dataset.rename(columns={
    'Sh_x': 'Shots_blocked',
    'Pass': 'Pass_blocked',
    'Int': 'Interceptions',
    'Clr': 'clearance',
    'Err': 'Mistakes',
    'Cmp%': 'pass_completion_%',
    'Sh_y': 'Sh'
})
player_dataset.replace([np.inf, -np.inf], np.nan, inplace=True)
player_dataset.fillna(0, inplace=True)

# Defining the function to convert the 'Annual Wages' and 'Weekly Wages' column into numerical value
def convert_pound_wage(value):
    """
    Convert a wage value string with currency symbols and commas to a numeric float.
    Extracts the British Pound (£) value and converts it to a float. If conversion fails, return NaN.
    """
    try:
        if isinstance(value, str) and '£' in value:
            pound_value = value.split('£')[1].split('(')[0]
            return float(pound_value.replace(',', '').strip())
    except (ValueError, IndexError):
        return np.nan
    return value

# Apply the conversion function to the 'Annual Wages' and 'Weekly Wages' column
player_dataset['Annual Wages'] = player_dataset['Annual Wages'].apply(convert_pound_wage)
player_dataset['Weekly Wages'] = player_dataset['Weekly Wages'].apply(convert_pound_wage)

player_dataset_copy = player_dataset.copy()

# Save the final table to a new Excel file
player_dataset.to_excel('/content/drive/My Drive/final_table.xlsx', index=False)

# Displaying the final dataframe
print(player_dataset.head())

# Checking for non-numeric values and handle them
def convert_to_numeric(value):
    """
    Converts values to numeric if possible, or returns NaN for non-numeric values.
    """
    try:
        return pd.to_numeric(value)
    except ValueError:
        return np.nan

# Applying the function to all columns that should be numeric
for col in player_dataset.columns:
    if player_dataset[col].dtype == 'object':
        player_dataset[col] = player_dataset[col].replace('90s', np.nan)
        player_dataset[col] = player_dataset[col].apply(convert_to_numeric)

# Handling missing values for numeric columns by filling with mean
numeric_cols = player_dataset.select_dtypes(include=[np.number]).columns
player_dataset[numeric_cols] = player_dataset[numeric_cols].fillna(player_dataset[numeric_cols].mean())

# Handling missing values for non-numeric columns by filling with mode
non_numeric_cols = player_dataset.select_dtypes(exclude=[np.number]).columns
for col in non_numeric_cols:
    player_dataset[col] = player_dataset[col].fillna(player_dataset[col].mode()[0])

# Encoding categorical features
label_encoder = LabelEncoder()
player_dataset['Pos'] = label_encoder.fit_transform(player_dataset['Pos'])
player_dataset['Squad'] = label_encoder.fit_transform(player_dataset['Squad'])

# Updating numerical features
numerical_features = ['Age', 'SCA', 'SCA90', 'GCA90', 'Blocks', 'Interceptions', 'clearance', 'pass_completion_%', 'Weekly Wages', 'Annual Wages']
numerical_features = [feature for feature in numerical_features if feature in player_dataset.columns]  # Ensure columns exist

# Exploratory Data Analysis (EDA)
# Checking the basic statistics
eda_summary = player_dataset.describe()
numeric_cols = player_dataset.select_dtypes(include=[np.number])
# Excluding 'Player', 'Squad', and 'Pos' columns from the correlation matrix
numeric_cols_eda = player_dataset.drop(['Player', 'Squad', 'Pos'], axis=1).select_dtypes(include=[np.number])
# Visualizing the correlations between performance metrics
plt.figure(figsize=(12, 8))
sns.heatmap(numeric_cols_eda.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()


#Plotting the histograms of key features
player_dataset.hist(bins=30, figsize=(18, 12))
plt.suptitle('Histograms of Key Features')
plt.show()

# Displaying basic info about the dataset
print("Basic Information:")
print(player_dataset.info())

print("\nDescriptive Statistics:")
print(player_dataset.describe())

#Identifing the correct target variable column name
if 'Pos' in player_dataset.columns:
    y = player_dataset['Pos']
else:
    raise ValueError(f"The target column 'Pos' is missing from the dataset.")

# Feature Scaling
scaler = StandardScaler()
player_dataset[numerical_features] = scaler.fit_transform(player_dataset[numerical_features])

# Feature Engineering
if 'Blocks' in player_dataset.columns and 'Interceptions' in player_dataset.columns and 'clearance' in player_dataset.columns:
    player_dataset['defensive_score'] = player_dataset['Blocks'] + player_dataset['Interceptions'] + player_dataset['clearance']
if 'SCA90' in player_dataset.columns and 'GCA90' in player_dataset.columns:
    player_dataset['offensive_score'] = player_dataset['SCA90'] + player_dataset['GCA90']

#Assigning New 'Future' Based on Broader Criteria
if 'offensive_score' in player_dataset.columns and 'defensive_score' in player_dataset.columns and 'Weekly Wages' in player_dataset.columns and 'Age' in player_dataset.columns:
    # Use percentiles to set thresholds
    offensive_threshold = np.percentile(player_dataset['offensive_score'], 70)  # Top 30% players
    defensive_threshold = np.percentile(player_dataset['defensive_score'], 70)  # Top 30% players
    wage_threshold = np.percentile(player_dataset['Weekly Wages'], 40)  # Bottom 40% wages
    age_threshold = np.percentile(player_dataset['Age'], 60)  # Top 40% younger players

    # Assign 'look for better option' if a player is above thresholds for performance and below for wages
    player_dataset['future'] = np.where(
        ((player_dataset['offensive_score'] > offensive_threshold) |
         (player_dataset['defensive_score'] > defensive_threshold)) &
        (player_dataset['Weekly Wages'] < wage_threshold) &
        (player_dataset['Age'] < age_threshold),
        'look for better option',
        'stay in the club'
    )
elif 'Age' in player_dataset.columns and 'Weekly Wages' in player_dataset.columns:
    print("Relevant columns not found or insufficient data, using broader alternative criteria.")

    # Broader criteria using median values and other conditions
    median_age = player_dataset['Age'].median() if 'Age' in player_dataset.columns else None
    median_wages = player_dataset['Weekly Wages'].median() if 'Weekly Wages' in player_dataset.columns else None

    # Assign 'look for better option' based on available features
    player_dataset['future'] = np.where(
        ((player_dataset['Age'] < median_age) & (player_dataset['Weekly Wages'] < median_wages)) |
        ('offensive_score' in player_dataset.columns and (player_dataset['offensive_score'] > player_dataset['offensive_score'].median())),
        'look for better option',
        'stay in the club'
    )
else:
    print("Insufficient data to create 'future' predictions.")

# Ensure the 'future' column has both categories
if 'future' in player_dataset.columns and player_dataset['future'].nunique() <= 1:
    raise ValueError("Target variable has only one class after assigning criteria. Please revise the 'future' assignment criteria to create a balanced dataset.")

# Check new target distribution
print("New target variable distribution after revised criteria:")
if 'future' in player_dataset.columns:
    print(player_dataset['future'].value_counts())

# Encode target variable after reassignment
if 'future' in player_dataset.columns:
    y = player_dataset['future'].apply(lambda x: 1 if x == 'look for better option' else 0)  # Encode target variable

# Step 3: Feature Selection using Mutual Information
# ----------------------------------------------
# Check if 'Player' and 'future' columns exist before dropping them
columns_to_drop = [col for col in ['Player', 'future'] if col in player_dataset.columns]

# Selecting top features using mutual information
X = player_dataset.drop(columns=columns_to_drop)  # Drop columns only if they exist

print("\nTarget variable distribution (after encoding):")
print(pd.Series(y).value_counts())  # Check distribution again

if y.nunique() <= 1:
    raise ValueError("Target variable has only one class after encoding. Please revise the 'future' assignment criteria.")

best_features = SelectKBest(score_func=mutual_info_classif, k='all')  # Use mutual information for feature selection
fit = best_features.fit(X, y)
dfscores = pd.DataFrame(fit.scores_)
dfcolumns = pd.DataFrame(X.columns)

# Concat two dataframes for better visualization
feature_scores = pd.concat([dfcolumns, dfscores], axis=1)
feature_scores.columns = ['Specs', 'Score']  # Naming the dataframe columns
print(feature_scores.nlargest(10, 'Score'))  # Print 10 best features


# Step 4: Handling Missing Values and Model Building
# ----------------------------------------------
# Impute missing values for numeric features using the mean
numeric_features = X.select_dtypes(include=[np.number]).columns
if len(numeric_features) > 0:  # Ensure there are numeric features
    numeric_imputer = SimpleImputer(strategy='mean')
    X[numeric_features] = numeric_imputer.fit_transform(X[numeric_features])

# Impute missing values for categorical features using the most frequent value
categorical_features = X.select_dtypes(exclude=[np.number]).columns
if len(categorical_features) > 0:  # Ensure there are categorical features
    categorical_imputer = SimpleImputer(strategy='most_frequent')
    X[categorical_features] = categorical_imputer.fit_transform(X[categorical_features])

# Check if there are any remaining NaN values in the dataset after imputation
if X.isna().sum().sum() > 0:
    raise ValueError("There are still NaN values in the dataset after imputation. Please check the data preprocessing steps.")


# Split data into training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model 1: XGBoost Classifier
model_xgb = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)
model_xgb.fit(X_train, y_train)

# Model 2: RandomForest Classifier
model_rf = RandomForestClassifier(n_estimators=100, random_state=42)
model_rf.fit(X_train, y_train)

# Custom Model: Threshold-Based Classifier
class CustomThresholdClassifier(BaseEstimator, ClassifierMixin):
    def __init__(self, threshold=0.5):
        self.threshold = threshold

    def fit(self, X, y):
        self.mean_ = X.mean(axis=0)  # Calculate mean of each feature
        self.thresholds_ = X.mean(axis=0) * self.threshold  # Define thresholds based on the mean
        return self

    def predict(self, X):
        # Simple prediction logic: If feature value is above a threshold, classify as 1, else 0
        predictions = np.where(X > self.thresholds_, 1, 0)
        # Aggregate by sum and threshold to decide class
        return (predictions.sum(axis=1) > (X.shape[1] / 2)).astype(int)

# Initialize and train the custom model
custom_model = CustomThresholdClassifier(threshold=0.5)
custom_model.fit(X_train, y_train)

# Evaluate Models
def evaluate_model(model, X_test, y_test, model_name="Model"):
    print(f"\nEvaluation for {model_name}:")
    y_pred = model.predict(X_test)
    print("Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred))
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))
    print(f"Accuracy Score: {accuracy_score(y_test, y_pred):.4f}")

# Evaluate XGBoost Model
evaluate_model(model_xgb, X_test, y_test, model_name="XGBoost Classifier")

# Evaluate RandomForest Model
evaluate_model(model_rf, X_test, y_test, model_name="RandomForest Classifier")

# Evaluate Custom Model
evaluate_model(custom_model, X_test, y_test, model_name="Custom Threshold Classifier")

# Step 7: Model Interpretation using SHAP (for XGBoost only)
# ----------------------------------------------
explainer = shap.Explainer(model_xgb)
shap_values = explainer(X_test)

# SHAP summary plot
shap.summary_plot(shap_values, X_test, plot_type="bar")

# SHAP dependence plot for the top feature
top_feature = X.columns[np.argmax(np.abs(shap_values.values).mean(0))]
shap.dependence_plot(top_feature, shap_values.values, X_test)

# Get feature importance from XGBoost model
feature_importances = model_xgb.feature_importances_
features = X.columns

# Create a DataFrame for visualization
importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Step 8: Adding Predictions to the Data
# ----------------------------------------------
# Predict future for the entire dataset using each model
player_dataset['future_xgb'] = model_xgb.predict(X)
player_dataset['future_rf'] = model_rf.predict(X)
player_dataset['future_custom'] = custom_model.predict(X)

# Map the predictions back to "stay in the club" or "look for better options"
player_dataset['future_xgb'] = player_dataset['future_xgb'].apply(lambda x: 'look for better option' if x == 1 else 'stay in the club')
player_dataset['future_rf'] = player_dataset['future_rf'].apply(lambda x: 'look for better option' if x == 1 else 'stay in the club')
player_dataset['future_custom'] = player_dataset['future_custom'].apply(lambda x: 'look for better option' if x == 1 else 'stay in the club')


# Align the player column by index
player_column = player_dataset_copy['Player'].reset_index(drop=True)

# Insert the player column at the beginning of the player_dataset
player_dataset.insert(0, 'player', player_column)


# Reorder columns to have 'Player' as the first column
output_columns = ['Player'] + [col for col in player_dataset.columns if col != 'Player']

# Save the updated dataset with predictions to a new Excel file
output_path = '/content/drive/My Drive/final_table_with_predictions.xlsx'
player_dataset[output_columns].to_excel(output_path, index=False)
print(f"Predictions with player names have been saved to {output_path}")